{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arthy2005/NanoGPT_built_GPT-style_Transformer_from_scratch/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDBOTnVjAIWl",
        "outputId": "6613c25a-143d-462f-f3e7-e596cb0293be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-08 14:54:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2026-01-08 14:54:06 (30.2 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IbDD3aE8fym"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text)) # 1,115,394 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nh0cNR-A4r1",
        "outputId": "eb86bdbe-4e3c-4997-b9af-a245976bf427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4NGdLL0VBurr",
        "outputId": "ff565e98-b846-4875-f6a8-783388839714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converts text into a set like {b, a, n} for set(\"banana\") which will get converted into list then into a sorted list\n",
        "print(set(\"banana\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJLBlTWdDLYj",
        "outputId": "c9400d82-2e11-4510-d8ae-c00246f33745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a', 'b', 'n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(text)))  # converts text into a set like {b, a, n} for set(\"banana\") which will get converted into list then into a sorted list\n",
        "print(''.join(characters))\n",
        "print(len(characters))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LVT0fSjCQkR",
        "outputId": "fb3ae941-0613-451c-b0d0-d10c12aaff18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need an encoder/decoder in order to encode text into integers and vise versa"
      ],
      "metadata": {
        "id": "odzKtAsqE8jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_integer = {ch:i for i,ch in enumerate(characters)}  # characters: corresponding integer/index  pairs for all characters\n",
        "integer_to_string = {i:ch for i,ch in enumerate(characters)}  #  integer/index: corresponding characters  pairs for all characters\n",
        "\n",
        "def encode(string):\n",
        "  list = []\n",
        "  for char in string:\n",
        "    list.append(string_to_integer[char])\n",
        "  return list\n",
        "\n",
        "def decode(list):\n",
        "  string = ''\n",
        "  for element in list:\n",
        "    string = string + (integer_to_string[element])\n",
        "  return string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpGnKUxnDG9I",
        "outputId": "a1bb7218-2893-4721-85ee-910ccb3c3a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # After creating the encoder/decoder now we have to encode our text\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KLLCPAj8KIMX",
        "outputId": "a45c215a-f74f-4be4-9798-7c0589629ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting our data for training and validation 90/10 % split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "wgATBDlmMMkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Transformer is a neural network architecture designed to understand sequences (text, code, DNA, audio).\n",
        "\n",
        "Instead of reading text one word at a time (like old RNNs), it looks at many tokens at once and decides what is important using attention.\n",
        "\n",
        "Thats why it becomes necessary to limit text to a block/chunk which we will use for training transformer."
      ],
      "metadata": {
        "id": "YuW2Gz1hP24d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaIRxAkXPRA8",
        "outputId": "8bf41ce6-08d4-44b0-d084-767562f08532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we want to do is as follows:\n",
        "\n",
        "  In the context of 18 -  47 comes next.\n",
        "\n",
        "  In the context of 18, 47 - 56 comes next.\n",
        "\n",
        "  In the context of... etc,\n",
        "  \n",
        "  This is because we want the transformer to see the patterns using a token upto a block_size."
      ],
      "metadata": {
        "id": "hM34D97MR0v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[:block_size+1]\n",
        "for t in range(block_size):\n",
        "  print(f\"In the context of {x[:t+1]} - {y[t+1]} comes next \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UYzsu9OR_r-",
        "outputId": "c0e7ac7b-9e0a-4b19-aea0-913254cd7bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the context of tensor([18]) - 47 comes next \n",
            "In the context of tensor([18, 47]) - 56 comes next \n",
            "In the context of tensor([18, 47, 56]) - 57 comes next \n",
            "In the context of tensor([18, 47, 56, 57]) - 58 comes next \n",
            "In the context of tensor([18, 47, 56, 57, 58]) - 1 comes next \n",
            "In the context of tensor([18, 47, 56, 57, 58,  1]) - 15 comes next \n",
            "In the context of tensor([18, 47, 56, 57, 58,  1, 15]) - 47 comes next \n",
            "In the context of tensor([18, 47, 56, 57, 58,  1, 15, 47]) - 58 comes next \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time dimension : In sequence modeling, Tokens are treated as events happening overtime, even if data is text.\n",
        "\n",
        " Each token = One time step\n",
        "\n",
        " 8 tokens = 8 steps in time\n",
        "\n",
        "  Time Dimension = Block Size"
      ],
      "metadata": {
        "id": "v6KEs7oCW8XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since GPUs have to power to process data parallely , we can train on multiple chunks at the same time. This adds an extra dimension which is batch."
      ],
      "metadata": {
        "id": "Ios-B6ydWIG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  if split == 'train':\n",
        "    data = train_data\n",
        "  else:\n",
        "    data = val_data\n",
        "  rand_start_pos = torch.randint(len(data) - block_size, (batch_size,)) #This create batch_size amount of random starting positions in our data for processing at the same time\n",
        "\n",
        "  x = torch.stack([data[i:i+block_size] for i in rand_start_pos])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in rand_start_pos])\n",
        "  return x, y\n",
        "\n",
        "\n",
        "x_inputs, y_targets = get_batch('train')\n",
        "print('inputs:')\n",
        "print(x_inputs)\n",
        "print('targets:')\n",
        "print(y_targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNN5dL8_YMhY",
        "outputId": "32bd69f2-912b-4c0b-96d4-6f7087d8c610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "len(data) - block_size  ensures we don't go out of bounds when slicing\n",
        "\n",
        "1 row contains 8 examples due to the time dimensions, so 4 rows contains 32 examples for the transformer"
      ],
      "metadata": {
        "id": "jOKaFXF0ba2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1 of Biagram Language Model\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "vocab_size = len(characters)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # a table of curr_char(a) and next char([2.1,-1.3,0.5.......,2.2])  where each row says If I see this character, how likely is every possible next character?\n",
        "\n",
        "  def forward(self, idx, targets):\n",
        "    # idx(indices of token ) and targets are both (B mens batch, T means time) tensors of integers\n",
        "    logits = self.token_embedding_table(idx) # logits is Raw prediction scores for each possible next token\n",
        "\n",
        "    return logits\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out = m(x_inputs, y_targets)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYWx_e59cscb",
        "outputId": "0d7a0a1f-f6cc-44a0-db65-28a32d50ab5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above 65 is channel dimension , what happens is each integer in our x_inputs for example lets take 5 , will go to our token_embedding_table and pluck out row 5 which will have 65 values corresponding to probability of next character/token corresponding to 5 , this will happen for all integers and a new tensor of [B, T, C] is formed.\n",
        "\n",
        "In Biagram Language Model the next character depends only on the current character."
      ],
      "metadata": {
        "id": "vdfQH1w8l1zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "vocab_size = len(characters)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # a table of curr_char(a) and next char([2.1,-1.3,0.5.......,2.2])  where each row says If I see this character, how likely is every possible next character?\n",
        "\n",
        "  def forward(self, idx, targets):\n",
        "    # idx(indices of token ) and targets are both (B mens batch, T means time) tensors of integers\n",
        "    logits = self.token_embedding_table(idx) # logits is Raw prediction scores for each possible next token\n",
        "\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B*T, C) # Stretching the 3D tensor into a 2D one\n",
        "    targets = targets.view(B*T)\n",
        "    loss = F.cross_entropy(logits, targets) # Pytorch need Channel dimension to be the second arguments, thus we had to do logits.view(B*T,C)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(x_inputs, y_targets)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHKfPfAXmlES",
        "outputId": "d246f959-733a-4c3b-e286-03fab87b5722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "negative log likelihood\n",
        "\n",
        "-ln(1/65) = 4.174\n"
      ],
      "metadata": {
        "id": "bjIRo6r6p27u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "vocab_size = len(characters)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # a table of curr_char(a) and next char([2.1,-1.3,0.5.......,2.2])  where each row says If I see this character, how likely is every possible next character?\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx(indices of token ) and targets are both (B mens batch, T means time) tensors of integers\n",
        "    logits = self.token_embedding_table(idx) # logits is Raw prediction scores for each possible next token\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # Stretching the 3D tensor into a 2D one\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets) # Pytorch need Channel dimension to be the second arguments, thus we had to do logits.view(B*T,C)\n",
        "\n",
        "    return logits, loss\n",
        "# Write new text, one token at a time\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens): # becomes (B, C)\n",
        "      logits, loss = self(idx) # Gets predictions for every token position\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim=-1) # Converts raw scores(logits) into probablities # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # Randomly choose/samples the next token on probabilities # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)#(B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(x_inputs, y_targets)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x-tHc5lpoQ7",
        "outputId": "5669c5f0-624e-40a9-eb8a-1a5172110329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is not trained yet, it works on random values and since its a bigram language model it only use the last character for guess."
      ],
      "metadata": {
        "id": "tjR9dKiNuh0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "vVlCptzAun_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  x_inputs, y_targets = get_batch('train') #choose/sample a batch of data\n",
        "  logits, loss = m(x_inputs, y_targets) # evaluate the loss\n",
        "  optimizer.zero_grad(set_to_none=True) #pytorch accumulates gradients so old gradients must be cleared before computing new ones\n",
        "  loss.backward() #backpropogation\n",
        "  optimizer.step()#uses gradient to move towards lower loss\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLVgnwlVvQjd",
        "outputId": "781e52db-024f-408c-a31b-7eacaed3cc60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.382369041442871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJOvMDFXw8z1",
        "outputId": "5b1eab5e-5ddc-47a8-ed35-5b07ee82defb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
            "RIfans picspeserer hee tha,\n",
            "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
            "ELIS me hurf lal y, ma dus pe athouo\n",
            "BEY:! Indy; by s afreanoo adicererupa anse tecor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above model is the simplest model, training decreased the loss by almost 2 units."
      ],
      "metadata": {
        "id": "VgAn_yQaxZrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SELF ATTENTION"
      ],
      "metadata": {
        "id": "vAogKGYsyzcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toy Example**"
      ],
      "metadata": {
        "id": "bfvOYz2HzE3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t_-Ak_bz0oI",
        "outputId": "4569d891-b451-4dee-e115-cdbab29fd841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The point here is that we shouldn't be able to communicate past our token/time , previous token should communicate some info to our current token, info flows from previous contextto current time step, one of the simplest way is for every single batch indepently for the tth token in a sequence we can take average of all the vectors in all the previous tokens and the current token but it looses the spacial arrangements of the tokens. but will ttry to bring it later."
      ],
      "metadata": {
        "id": "95WT9L2S1LQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b, i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] # (t,C)\n",
        "    xbow[b,t] = torch.mean(xprev,0) # effectively taking a mean of time dimension"
      ],
      "metadata": {
        "id": "1HmwFXuy1K_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A trick in self attention**\n",
        "its nothing but a matrix multiplication on unit lower trinagular matrix with another matrix which results in the average of elements in columns iteratively like we wanted."
      ],
      "metadata": {
        "id": "rh0QLR5j5DpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"c=\")\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yO74gfV0hmt",
        "outputId": "06a1511a-9e4c-4836-82b9-b4c8a69f0f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "# what if we change this a\n",
        "a = a/ torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"c=\")\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAUqgOKP6Ni_",
        "outputId": "4e29a31e-9978-4565-9be4-07e9583ae5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "at row 2 we get avg of col1 till row2 and col2 till row2.\n",
        "\n",
        "at row 3 we get avg of col1 and col2"
      ],
      "metadata": {
        "id": "Axy6BAGm6g6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 2\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei/ wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n"
      ],
      "metadata": {
        "id": "1avQ29B26lqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xbow)\n",
        "print(xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TpVxZNJ6k4y",
        "outputId": "5490d36d-e334-40de-e960-dd1fccf387aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 3: Use Softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T)) #affinities\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) #future can't communicate with the past\n",
        "wei = F.softmax(wei, dim=-1)#normalise\n",
        "xbow3 = wei @ x #sum\n",
        "torch.allclose(xbow2, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IWl1UNS7cde",
        "outputId": "c135b575-f576-4773-8199-0bbaf2d8d22d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "rSCMtGs499i-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c628142-1684-43ba-c45a-585293648616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7rbxspYcfQa",
        "outputId": "ad98bcda-c3a9-4e1a-f303-3f0b104bb9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For decreasing the variance to around 1\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "nHcKvFvTcf33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzYKpTF2c7_Y",
        "outputId": "af560c39-c2f4-4c33-f5fd-a78445f2ff13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhlJnBw3dOj2",
        "outputId": "fe2a7ecc-706f-46ac-9678-4b5cf7f513c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIw7rUP5dPRH",
        "outputId": "8ff27249-e09d-4c41-8d26-e194871751ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lets Build our code based on the things we learnt\n",
        "\n",
        "Lets also improve the model we use to for better results as per the paper **Attention Is All You Need.**"
      ],
      "metadata": {
        "id": "2tMf1KVqhoWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modules to import**"
      ],
      "metadata": {
        "id": "EaAe4Cfrh14q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "aHgqdKrVdRoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**"
      ],
      "metadata": {
        "id": "wz2xuEKrru92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "ZMC2HKGiij4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some new hyperparameters"
      ],
      "metadata": {
        "id": "3nwNPKMS5gbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 100\n",
        "eval_iters = 200\n",
        "n_embd = 64 # no. of embeddings\n",
        "n_head = 4 # no heads for multi head\n",
        "n_layer = 4 #no. of layers\n",
        "dropout = 0.0 #dropout for droping some weights during training"
      ],
      "metadata": {
        "id": "WP1-QV6SsAjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to add a new variable called device , so that we can run our code on gpu"
      ],
      "metadata": {
        "id": "f-nulOqa8Mgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "w7g09tcw8Us8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After doing the above we have to pass the data we calculate to the device so that our gpu can perform calculations"
      ],
      "metadata": {
        "id": "p3JIfHRg8dow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) # for same random values every time we run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRSBgdJY7BXA",
        "outputId": "5cdce5e8-ed7b-457c-ecf3-c89136d76805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ffa70782ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input.txt file we downloaded from Andrej Karpathy\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "Bp0pXgJN7JOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our encoder decoder we built\n",
        "string_to_integer = {ch:i for i,ch in enumerate(characters)}  # characters: corresponding integer/index  pairs for all characters\n",
        "integer_to_string = {i:ch for i,ch in enumerate(characters)}  #  integer/index: corresponding characters  pairs for all characters\n",
        "\n",
        "def encode(string):\n",
        "  list = []\n",
        "  for char in string:\n",
        "    list.append(string_to_integer[char])\n",
        "  return list\n",
        "\n",
        "def decode(list):\n",
        "  string = ''\n",
        "  for element in list:\n",
        "    string = string + (integer_to_string[element])\n",
        "  return string"
      ],
      "metadata": {
        "id": "6FuqMPiL7Wva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting our data for training and validation 90/10 % split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "vBZWZdGu7ogJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loading**"
      ],
      "metadata": {
        "id": "y3MOGwIO783K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)  # device variable we built at hyperparameters section\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "jo-T67Bo7_cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mutli Head Self-Attention\n",
        "\n",
        "we can use our bigram language model here to build a model , but we can optimise it further using **Multi head self attention**"
      ],
      "metadata": {
        "id": "_JXiFqop-1-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **#Single Head Self-Attention**\n",
        "\n",
        "  A mechanism where each token attends to all other tokens in the same sequence using one set of Query, Key, and Value projections to produce  context-aware representation\n",
        "\n",
        "  we can build multi head self attention by iterating over single head self attention."
      ],
      "metadata": {
        "id": "LeUeqjol_oRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Each token computes attention once, using one set of parameters.\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "eKQAZsC5_m3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi Head Self Attention** = For loop of Single head self attention upto head_size times"
      ],
      "metadata": {
        "id": "DWPWifHp_6gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "_E0QGnJ2BIm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Block\n",
        "\n",
        "Think of it as one layer that takes in embeddings and outputs better, context-aware embeddings.\n",
        "\n",
        "It contains Self-attention, Feed forward and layer norm"
      ],
      "metadata": {
        "id": "zLd6o0sqBeee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "gNrHhlznBNlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use this Trasnformer Block in our model to get better results then our simple model we made earlier"
      ],
      "metadata": {
        "id": "wC_lYzgeCbBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Better Bigram Language Model"
      ],
      "metadata": {
        "id": "JsgTI3GoCxyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DrnOoLl8-UTD",
        "outputId": "b21f8f60-fcb3-4cae-a555-58ec2281179a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimisation\n",
        "\n",
        "We can use the one we used to before"
      ],
      "metadata": {
        "id": "y0vfXHNdDu8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "yGSrB6kXDmwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to write a simple function which average outs the loss value we get after multiple iterations, so that we can understand the values of loss consistently"
      ],
      "metadata": {
        "id": "swksXydg8xOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() #prevents spending of computational resources which would have been allocated by pythorch\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() #model will be defined later when we use bigram language model\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "7XTmkipo8J36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train') # Similar to x_inputs, y_targets we used in our code\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3wYSz0zEFgBA",
        "outputId": "ef82b820-cd49-4f31-ffb2-95410cc1a966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5058\n",
            "step 300: train loss 2.4197, val loss 2.4336\n",
            "step 400: train loss 2.3501, val loss 2.3562\n",
            "step 500: train loss 2.2963, val loss 2.3125\n",
            "step 600: train loss 2.2407, val loss 2.2496\n",
            "step 700: train loss 2.2054, val loss 2.2187\n",
            "step 800: train loss 2.1633, val loss 2.1866\n",
            "step 900: train loss 2.1241, val loss 2.1504\n",
            "step 1000: train loss 2.1036, val loss 2.1306\n",
            "step 1100: train loss 2.0698, val loss 2.1180\n",
            "step 1200: train loss 2.0380, val loss 2.0791\n",
            "step 1300: train loss 2.0248, val loss 2.0634\n",
            "step 1400: train loss 1.9926, val loss 2.0359\n",
            "step 1500: train loss 1.9697, val loss 2.0287\n",
            "step 1600: train loss 1.9627, val loss 2.0477\n",
            "step 1700: train loss 1.9403, val loss 2.0115\n",
            "step 1800: train loss 1.9090, val loss 1.9941\n",
            "step 1900: train loss 1.9092, val loss 1.9858\n",
            "step 2000: train loss 1.8847, val loss 1.9925\n",
            "step 2100: train loss 1.8724, val loss 1.9757\n",
            "step 2200: train loss 1.8580, val loss 1.9594\n",
            "step 2300: train loss 1.8560, val loss 1.9537\n",
            "step 2400: train loss 1.8412, val loss 1.9427\n",
            "step 2500: train loss 1.8141, val loss 1.9402\n",
            "step 2600: train loss 1.8292, val loss 1.9397\n",
            "step 2700: train loss 1.8116, val loss 1.9322\n",
            "step 2800: train loss 1.8032, val loss 1.9218\n",
            "step 2900: train loss 1.8022, val loss 1.9285\n",
            "step 3000: train loss 1.7955, val loss 1.9195\n",
            "step 3100: train loss 1.7672, val loss 1.9192\n",
            "step 3200: train loss 1.7568, val loss 1.9138\n",
            "step 3300: train loss 1.7551, val loss 1.9059\n",
            "step 3400: train loss 1.7549, val loss 1.8945\n",
            "step 3500: train loss 1.7383, val loss 1.8956\n",
            "step 3600: train loss 1.7242, val loss 1.8868\n",
            "step 3700: train loss 1.7273, val loss 1.8822\n",
            "step 3800: train loss 1.7176, val loss 1.8923\n",
            "step 3900: train loss 1.7219, val loss 1.8750\n",
            "step 4000: train loss 1.7131, val loss 1.8603\n",
            "step 4100: train loss 1.7105, val loss 1.8777\n",
            "step 4200: train loss 1.7033, val loss 1.8675\n",
            "step 4300: train loss 1.7038, val loss 1.8556\n",
            "step 4400: train loss 1.7057, val loss 1.8643\n",
            "step 4500: train loss 1.6875, val loss 1.8528\n",
            "step 4600: train loss 1.6887, val loss 1.8405\n",
            "step 4700: train loss 1.6834, val loss 1.8501\n",
            "step 4800: train loss 1.6675, val loss 1.8437\n",
            "step 4900: train loss 1.6684, val loss 1.8407\n",
            "step 4999: train loss 1.6645, val loss 1.8286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finally we can generate output**"
      ],
      "metadata": {
        "id": "c-Mex1BWGG1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnmt-DMEGFmD",
        "outputId": "068590f5-1079-4a48-c595-4aeed4c75aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "Shal lifest made to bub, to take Our my dagatants:\n",
            "Whith foul his vetward that a endrer, my fears' to zorm heavens,\n",
            "Oof it heart my would but\n",
            "With ensengmin latest in ov the doest not.\n",
            "\n",
            "WARWICK:\n",
            "Welll now, and thus quechiry: there's speak you love.\n",
            "In Bodiet, and whom the sclittle\n",
            "Enout-now what evily well most rive with is compon to the me\n",
            "Town danters, If so;\n",
            "Ange to shall do aleous, for dear?\n",
            "\n",
            "KING HENRY VI:\n",
            "Hark, but a\n",
            "ards bring Edward?\n",
            "\n",
            "GROKE:\n",
            "As is no Rurnts I am you! who neet.\n",
            "Pom mary thou contrantym so a thense.\n",
            "\n",
            "QUEEN VINCENTIO:\n",
            "O, sir, may in God't well ow, whom confessy.\n",
            "Which migh.\n",
            "\n",
            "ARCHILINIUS:\n",
            "Dithul seaze Peed me: very it passce of's cruport;\n",
            "How what make you fear tals: there loves\n",
            "Tunkistren in deed, is xment.\n",
            "\n",
            "CORIONIUS:\n",
            "What comforts me. I with self From the walt I?\n",
            "\n",
            "GRINION:\n",
            "Which ushold.\n",
            "\n",
            "KING HENRY Gindner:\n",
            "Withrief I doot, is onter now.\n",
            "\n",
            "Securming:\n",
            "Intande whose no crown some Eiverely marry sold;\n",
            "For for me watch the\n",
            "our torguet! Goy, know our her and brut what I, I huself as humsell.\n",
            "\n",
            "APTOLYCUM:\n",
            "Laitance and toarth or word\n",
            "As beherefitions so me worting.\n",
            "\n",
            "CORIOLINA:\n",
            "What a wouldds,\n",
            "An but branedy wouldIng my a canity:\n",
            "Was you be any in Becausing watcess the Regreast men is what see would in thas jury your Hrannertandless;\n",
            "As there'erliacter me band frind through he crown, I she love is stay just torment:\n",
            "Slaw you behoth unserving of vonby the post,\n",
            "Whave baste hold; I they nengety may's fries\n",
            "To there's fince, I heave arrow old,\n",
            "Thee best sincess soul be\n",
            "that Lord, as;\n",
            "River thou a-latsteer:\n",
            "Out.\n",
            "\n",
            "PORALLINA:\n",
            "Where but\n",
            "Braight gentle, drieven the know you\n",
            "for that to this mack a rishn. Prawity arm as is infectely,\n",
            "Ah, sinstats o' no, this send; commant to love,\n",
            "Go fly this fathal\n",
            "I cortuns cold, offrong to old, the courtly thee? before a gace.\n",
            "\n",
            "KING RICHARD III:\n",
            "A life he pusict\n",
            "It. Vitters, and were not fanturs, thy promind thy awonse than a braute comforn,\n",
            "Will Roman! you brain shown'd for a dresss me; he heavison!\n",
            "\n",
            "\n",
            "MENE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Even Better Results You can  modify the hyperparameters depending on the GPU**"
      ],
      "metadata": {
        "id": "7bbbvAV_ImDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 64\n",
        "# block_size = 256\n",
        "# max_iters = 5000\n",
        "# eval_interval = 500\n",
        "# learning_rate = 3e-4\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# eval_iters = 200\n",
        "# n_embd = 384\n",
        "# n_head = 6\n",
        "# n_layer = 6\n",
        "# dropout = 0.2"
      ],
      "metadata": {
        "id": "TsgcLgvvIwAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}